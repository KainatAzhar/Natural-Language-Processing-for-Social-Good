{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# YOLOv5 Video Object Detection\n",
    "\n",
    "### Project Overview\n",
    "This notebook demonstrates how to perform object detection on a video file using the **YOLOv5** model. It processes a video frame by frame, detects objects in each frame, and saves the results as a new annotated video file. This project showcases proficiency in real-time object detection and video processing, which are crucial skills for computer vision applications.\n",
    "\n",
    "### Methodology\n",
    "1.  **Setup and Model Loading:** The notebook installs the necessary libraries and loads a pre-trained YOLOv5 model from PyTorch Hub.\n",
    "2.  **Video Processing:** It accesses a video file from Google Drive, reads it frame by frame using `OpenCV`, and runs the YOLOv5 model on each frame.\n",
    "3.  **Visualization:** It draws bounding boxes and labels on each frame to visualize the detected objects.\n",
    "4.  **Output Generation:** The processed frames are compiled and saved as a new video file in the user's Google Drive, making it easy to download and share.\n",
    "\n",
    "### Technologies Used\n",
    "- Python\n",
    "- PyTorch\n",
    "- OpenCV\n",
    "- YOLOv5"
   ],
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Section 1: Setup and Model Loading ---\n",
    "\n",
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "from google.colab import drive\n",
    "\n",
    "# Install the required library for YOLOv5\n",
    "try:\n",
    "    import ultralytics\n",
    "except ImportError:\n",
    "    print(\"Installing ultralytics library...\")\n",
    "    !pip install -U 'ultralytics'\n",
    "    import ultralytics\n",
    "\n",
    "print(\"Loading pre-trained YOLOv5 model from PyTorch Hub...\")\n",
    "try:\n",
    "    model = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True)\n",
    "    print(\"Model loaded successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading YOLOv5 model: {e}\")\n",
    "    raise # Re-raise the exception\n",
    "\n",
    "# Set model confidence and NMS thresholds\n",
    "model.conf = 0.4  # Set confidence threshold to 40%\n",
    "model.iou = 0.5   # Set IoU threshold for NMS to 50%\n",
    "\n",
    "# --- Section 2: Mount Google Drive and Specify Video Paths ---\n",
    "\n",
    "print(\"\\nMounting Google Drive to access video file...\")\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# IMPORTANT: Replace with the actual path to your video file in Google Drive\n",
    "video_path = '/content/drive/MyDrive/path_to_your_video.mp4'\n",
    "\n",
    "# IMPORTANT: Replace with the desired output path for the processed video\n",
    "output_video_path = '/content/drive/MyDrive/yolov5_output_video.mp4'\n",
    "\n",
    "print(f\"\\nInput video path: {video_path}\")\n",
    "print(f\"Output video path: {output_video_path}\")\n",
    "\n",
    "# --- Section 3: Video Object Detection ---\n",
    "\n",
    "print(\"\\nStarting video processing...\")\n",
    "\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Could not open video file.\")\n",
    "else:\n",
    "    # Get video properties\n",
    "    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "    \n",
    "    # Define the codec and create VideoWriter object to save the output\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v') # You can use other codecs like 'XVID'\n",
    "    out = cv2.VideoWriter(output_video_path, fourcc, fps, (frame_width, frame_height))\n",
    "    \n",
    "    frame_count = 0\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Perform inference\n",
    "        results = model(frame)\n",
    "        \n",
    "        # Render bounding boxes and labels on the frame\n",
    "        results.render()\n",
    "        annotated_frame = results.ims[0]\n",
    "        \n",
    "        # Write the annotated frame to the output video\n",
    "        out.write(cv2.cvtColor(annotated_frame, cv2.COLOR_RGB2BGR))\n",
    "        \n",
    "        frame_count += 1\n",
    "        print(f\"Processed frame: {frame_count}\", end='\\r')\n",
    "\n",
    "    # Release resources\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    print(\"\\n\\nVideo processing complete. The output video is saved to your Google Drive.\")"
   ]
  }
 ]
}
